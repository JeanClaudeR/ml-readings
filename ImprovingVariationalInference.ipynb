{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Variational Inference\n",
    "\n",
    "Key idea in variational inference is to approximate the true posterior with a variational distribution. This turns the inference problem to an optimization problem that minimizes the distance between these two distributions using a metric like KL divergence.\n",
    "\n",
    "<p align=\"center\"> \n",
    "<img src=\"images/vi-recap.png\" alt=\"Variational Inference\" width=\"350\"/>\n",
    "</p>\n",
    "\n",
    "In the above diagram $x$ is the data (e.g. image or a sound clip) that has been generated by an unknown latent factor $z$. Part of this problem is to learn $z$ and in doing that we need to approximate the true posterior with an approximate distribution.\n",
    "\n",
    "Let $q_{x}(z)$ be the approximation of the true posterior $p(z | x)$. And we pick KL divergence to minimize the distance between these 2 distributions.\n",
    "\n",
    "$\\begin{aligned} D_{\\mathrm{KL}}\\left[q_{x}(z) \\| p(z | x)\\right] &=\\mathbb{E}_{z \\sim q_{x}(z)}\\left[\\log q_{x}(z)-\\log p(z | x)\\right] \\\\ &=\\mathbb{E}_{z \\sim q_{x}(z)}\\left[\\log q_{x}(z)-\\log \\frac{p(z, x)}{p(x)}\\right] \\\\ &=\\mathbb{E}_{z \\sim q_{x}(z)}\\left[\\log q_{x}(z)-\\log p(z)-\\log p(x | z)+\\log p(x)\\right] \\\\ &=\\underbrace{\\mathbb{E}_{z \\sim q_{x}(z)}\\left[\\log q_{x}(z)-\\log p(z)-\\log p(x | z)\\right]}_{\\text {Only this part depends on } z} \\end{aligned}$\n",
    "\n",
    "Using simple algebra and Bayes' rule, we arrive at the above expression that can be used to minimize the distance between the distributions. The expectation in the expression can be approximated by stochastic samples and each term within the expectation can be computed in O(1) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference as Importance Sampling\n",
    "\n",
    "To train a latent variable model, we want to compute the marginal likelihood for any given $x$: $p(x)=\\sum_{z} p(z, x)$. And the way we train it is by using Maximum Likelihood. This means that for any given $x$ we need to compute the marginal probability $p(x)$. To get the marginal, assuming that we have the discrete latent code $z$, we need to sum over all $z$ for this joint probability $p(z, x)$. This becomes difficult if $z$ has exponential number of choices.\n",
    "\n",
    "But here's an intuition from empirical observations. For any $x$, typically $p(x, z)$ will have probability mass concentrated in very few places. e.g. if for an image the latent code $z$ is used to describe a semantic thing like if there is a car in the image, then only for that timy fraction of the image where the car is present will have that $z$ turned on.\n",
    "\n",
    "Hence in most of the VAE or any other meaningful latent variable models, we will have a very picky distribution in the joint space for any given $x$. This implies that in the high dimensional space we don't need to enumerate through all the possibilities of $z$. We can place emphasis on the possibilities of $z$ that are more likely under that $x$. This is one way of looking at _variational inference as Importance Sampling_.\n",
    "\n",
    "**Intuition:** The variational distribution $q(z | x)$ samples the high density region of $p(z, x)$\n",
    "\n",
    "Have a look at the following derivation:\n",
    "\n",
    "$\\begin{aligned} \\log p(x) &=\\log \\sum_{z} p(z, x) \\\\ &=\\log \\sum_{z} q(z | x) \\frac{p(z, x)}{q(z | x)} \\\\ &=\\log \\mathbb{E}_{z \\sim q(z | x)}\\left[\\frac{p(z, x)}{q(z | x)}\\right] \\\\ & \\geq \\mathbb{E}_{z \\sim q(z | x)}\\left[\\log \\frac{p(z, x)}{q(z | x)}\\right] \\end{aligned}$\n",
    "\n",
    "A few things to note here:\n",
    "\n",
    "* The expression in the third line is very close to the Variational Lower Bound (VLB). In fact if we get the $\\log$ inside by applying Jensen's inequality as in the fourth line then we get the VLB itself.\n",
    "* The third line of the derivation implies that if we draw a lot of samples from $q(z | x)$, average them and then take the $\\log$ we approach $\\log p(x)$.\n",
    "* The fourth line of the derivation gives the VLB. This says that if I push the $\\log$ inside I can draw _one_ sample $z$ and get the lower bound itself.\n",
    "\n",
    "\n",
    "So what the above implies is that Variational Inference is one way to do Importance Sampling. And that's exactly what we do in a VAE _with one sample_ (the 4th line in the derivation above). **The question is if we use multiple samples for importance sampling in a VAE, will that improve the lower bound ?**\n",
    "\n",
    "The answer is **Yes!** and Burda et al shows the way to do it in the paper [Importance Weighted Autoencoders](https://arxiv.org/abs/1509.00519) - by Yuri Burda, Roger Grosse & Ruslan Salakhutdinov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Improvements\n",
    "\n",
    "One of the ways that we can think of improving VAE is by reducing the gap between marginal log likelihood and the Variational Lower Bound, which implies reducing the mismatch between the approximate posterior and the true posterior. i.e. reducing $D_{\\mathrm{KL}}\\left[q_{x}(z | x) \\| p(z | x)\\right]$. We can do the following to reduce this gap:\n",
    "\n",
    "* Using Importance Sampling as per the paper [Importance Weighted Autoencoders](https://arxiv.org/abs/1509.00519) by Burda et al \n",
    "* More expressive approximate posterior $q(z | x)$ which will also result in the true posterior capable of expressing more forms\n",
    "* More expressive prior $p(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Importance Sampling\n",
    "\n",
    "As Burda et al suggests the idea is to draw $k$ samples (instead of 1) to form an approximation that is closer to the true marginal log likelihood. And just train on this objective:\n",
    "\n",
    "$\\mathcal{L}_{k}=\\mathbb{E}\\left[\\log \\frac{1}{k} \\sum_{i=1}^{k} w_{i}\\right] \\leq \\log \\mathbb{E}\\left[\\frac{1}{k} \\sum_{i=1}^{k} w_{i}\\right]=\\log p(\\mathbf{x})$ \n",
    "\n",
    "where\n",
    "\n",
    "$w_{i}=\\frac{p\\left(z_{i}, x\\right)}{q\\left(z_{i} | x\\right)}$\n",
    "\n",
    "Here we choose $k$ to be the number of samples that we can afford to draw and then train on this modified objective. When $k$ is large it's as if we are training on the true marginal log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparison of the improvements that we get using IWAE as compared to using plain VAE: (source Burda et al's paper)\n",
    "\n",
    "<p align=\"center\"> \n",
    "<img src=\"images/mnist-iwae.png\" alt=\"MNIST on IWAE\" width=\"350\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an illustration that shows how using IWAE can result in more expressive posteriors than using regular VAEs: (source Burda et al's paper)\n",
    "\n",
    "<p align=\"center\"> \n",
    "<img src=\"images/iwae-expressive-posteriors.png\" alt=\"Expressive Posteriors with VAE\" width=\"350\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above diagram, the heatmaps show the true posteriors $p(z | x)$ of 4 data points for VAE and IWAE models. The leftmost heatmap shows for a VAE and the other 2 shows the same for IWAE with $k = 5$ and $k = 50$ respectively.\n",
    "\n",
    "For the VAE case the figure shows that the posterior is (sort of) Gaussian, some circularish and some elongated and non axes aligned. Why is this so ?\n",
    "\n",
    "The setting that we have here is:\n",
    "\n",
    "* Prior $p(z)$ is $Normal(0, I)$\n",
    "* Approximate posterior (variational distribution) $q(z | x)$ is $Normal(\\mu, diag(\\sigma)$, where $\\mu$ and $\\sigma$ come from the output of a neural network. So approximate posterior also takes the shape of a diagonal Gaussian\n",
    "\n",
    "But none of the above explains why the _true posterior_ should also be Gaussian (non axes elongated). Then how can we explain this ?\n",
    "\n",
    "In a VAE when we maximize the variational lower bound (VLB), we are trying to maximize the marginal log likelihood, which also tries to minimize the KL divergence between the approximate and true posterior. This has 2 effects:\n",
    "\n",
    "* Pushes the approximate posterior to be closer to the true posterior\n",
    "* Pulls the true posterior to a form that can be expressed by the approximate posterior\n",
    "\n",
    "This means that in deep generative modeling with latent codes, the choice of the variational distribution also shapes the generative model. In a sense this forces the generative model to conform to the family of the variational distribution.\n",
    "\n",
    "As the IWAE paper suggests, in IWAE if we a larger $k$ we get to use a more expressive variational distribution $q$ which will also result in the true posterior being more expressive. So larger $k$ implies fewer restrictions on the true posterior. Have a look at the MNIST example above for the heatmaps corresponding to IWAE models. In the first row of the IWAE model with $k = 5$, we get an elongated Gaussian in a non axis aligned way. In the second row we get 2 modes for IWAE which is not possible just with a plain VAE. And with larger $k$ we get more exotic shapes.\n",
    "\n",
    "True posteriors can be complicated and if we assume the approximate posterior to be a Gaussian with diagonal covariance, then we can only express a limited form in the true posterior as with plain VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Expressive Posterior\n",
    "\n",
    "In the last section we saw one way of making posteriors more expressive by using importance sampling technique to implement an Importance Weighted Autoencoders. In this section we will discuss another approach based on some first principles way of thinking about posteriors. \n",
    "\n",
    "Say we have a fixed prior $p(z)$ for the latent space (e.g. an isotropic Gaussian). We can then think that the approximate posterior $q(z | x)$ has a _bin packing_ problem. For every data point $x$, the approximate posterior $q(z | x)$ maps $x$ to a distinct region in $p(z)$. So that $p(x | z)$ can reconstruct the data point with as little loss of information as possible. And more the packing of $x$ ito $p(z)$ the better. This is just an intuition - in reality however, we would like our model to generalize to data points that it has not seen so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following example where we fit a VAE with spherical Gaussian prior $p(z)$ and factorized Gaussian approximate posterior with diagonal covariance $q(z | x)$ to a toy data set of 4 points $\\{A, B, C, D\\}$:\n",
    "\n",
    "<p align=\"center\"> \n",
    "<img src=\"images/gaussian-prior-vae.png\" alt=\"VAE with Gaussian prior and posterior\" width=\"350\"/>\n",
    "</p>\n",
    "\n",
    "The left box shows the isotropic Gaussian prior and in the right box each colored cluster corresponds to the approximate posterior distribution of one datapoint e.g. $q(z | x = A)$. Note that all the $4$ posteriors model some form of isotropic Gaussian only which is not very expressive. The fact that we see lots of empty spaces in between the distributions also imply that the bin packing was poor that would lead to inefficient inference downstream.\n",
    "\n",
    "Now compare this with the following diagram where the rightmost box has a much more expressive posterior that allows for a much better fit between the prior and the posterior (aka better _bin packing_):\n",
    "\n",
    "<p align=\"center\"> \n",
    "<img src=\"images/gaussian-prior-iaf.png\" alt=\"IAF posterior\" width=\"350\"/>\n",
    "</p>\n",
    "\n",
    "Now it turns out that implementing such expressive posteriors is possible and we will discuss some of them in the following sections. The above diagram is from one such paper [Improved Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934) by Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever and Max Welling, and improves VAE with more flexible posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
